---
title: "On Transformers (with PyTorch)"
author: "Shamanth Kuthpadi S."
date: "2025-08-18"
categories: [machine learning, neural networks, architecture]
image: "081825_transformers.png"
draft: true
---

I’ve always found that building systems and documenting concepts from the ground up is the most effective way for me to learn—especially when it comes to complex architectures like transformers. This blog post is motivated by that philosophy: to create a comprehensive, self-contained record of everything I learn about transformers. By sharing this work, I hope to provide researchers and practitioners with a clear, consolidated reference that makes understanding and exploring transformer architectures more accessible.

### Architecture Diagram [1]
<img src="https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png" width="350">

### Why Transformers?

Originally, the transformer architecture was introduced for machine translation—the task of converting text from one language to another [1]. Since then, it has become remarkably ubiquitous across a wide range of domains.

Naturally, this raises an important question: why? To answer this we should begin by understanding the features of transformers that make them different from recurrent neural networks (i.e., long short-term memory and gated recurrent neural networks).


#### Recurrent Neural Networks
Prior to the introduction of transformers, recurrent neural nets were the state-of-the-art for sequence modeling. 

# References
[1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
